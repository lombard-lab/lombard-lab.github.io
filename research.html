<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Lombard Lab</title>
    <link rel="stylesheet" href="css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="container">
            <div class="logo">
                <h1>Lombard Lab</h1>
            </div>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="research.html" class="active">Research</a></li>
                    <li><a href="papers.html">Papers</a></li>
                    <li><a href="datasets.html">Datasets</a></li>
                </ul>
            </nav>
            <div class="mobile-menu-toggle">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </header>

    <main>
        <section class="page-header">
            <div class="container">
                <h1>Our Research</h1>
                <p>Exploring the frontiers of computer vision and video understanding</p>
            </div>
        </section>

        <section class="page-content">
            <div class="container">
                <div class="research-areas">
                    <div class="research-area">
                        <h2>Video Action Recognition</h2>
                        <p>We develop models that can accurately recognize and classify human actions in video sequences, even in complex real-world scenarios with multiple actors and occlusions.</p>
                        <div class="research-highlights">
                            <h3>Key Contributions</h3>
                            <ul>
                                <li>Temporal modeling architectures for long-range dependencies</li>
                                <li>Efficient video transformers for real-time applications</li>
                                <li>Multi-modal fusion techniques combining visual and audio cues</li>
                            </ul>
                        </div>
                    </div>

                    <div class="research-area">
                        <h2>Video Captioning and Description</h2>
                        <p>Our work focuses on generating natural language descriptions of video content, enabling applications in accessibility, video search, and content understanding.</p>
                        <div class="research-highlights">
                            <h3>Key Contributions</h3>
                            <ul>
                                <li>Attention mechanisms for temporal alignment</li>
                                <li>Dense captioning of video segments</li>
                                <li>Evaluation metrics for video description quality</li>
                            </ul>
                        </div>
                    </div>

                    <div class="research-area">
                        <h2>Video Temporal Localization</h2>
                        <p>We develop techniques to precisely locate and segment actions or events within longer video sequences, enabling fine-grained video understanding.</p>
                        <div class="research-highlights">
                            <h3>Key Contributions</h3>
                            <ul>
                                <li>Weakly-supervised temporal action localization</li>
                                <li>Boundary-aware networks for precise segmentation</li>
                                <li>Multi-scale temporal modeling</li>
                            </ul>
                        </div>
                    </div>

                    <div class="research-area">
                        <h2>Video-based Forecasting</h2>
                        <p>Our research explores predicting future events and actions from video observations, with applications in autonomous systems, surveillance, and human-robot interaction.</p>
                        <div class="research-highlights">
                            <h3>Key Contributions</h3>
                            <ul>
                                <li>Probabilistic forecasting models</li>
                                <li>Anticipating human activities and interactions</li>
                                <li>Long-term video prediction</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Lombard Lab</h3>
                    <p>Advancing computer vision through innovative research and open datasets.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="research.html">Research</a></li>
                        <li><a href="papers.html">Publications</a></li>
                        <li><a href="datasets.html">Datasets</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Contact</h4>
                    <p>Email: contact@lombard-lab.org</p>
                    <p>GitHub: <a href="https://github.com/lombard-lab">lombard-lab</a></p>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2023 Lombard Lab. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="js/script.js"></script>
</body>
</html>